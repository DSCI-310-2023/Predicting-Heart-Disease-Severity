{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING ACCURACY OF PREDETERMINED VARIABLES TO PREDICT SEVERITY OF HEART DISEASE IN PATIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapted by:** Daniel Lee, Eric Leung, & Sam Thorne\n",
    "\n",
    "**Original Authors:** Emerson Crick, Allie Janowicz, Ziva Subelj, & Sam Thorne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart disease refers to a number of different cardiovascular conditions including coronary artery disease, arrhythmia, and heart failure. These diseases have become the leading causes of death in the United States, killing over 659,000 people and costing the government $363 billion each year. Heart disease is associated with a number of factors such as unhealthy blood pressure, cholesterol, and more {cite}`disease_control`. \n",
    "\n",
    "\n",
    "**We are asking: are age (`age`), maximum heart rate (`max_heart_rate`), and resting blood pressure (`rest_bp`) good at predicting the severity of heart disease in a patient? Based on literature research, we predict that age, maximum heart rate, and resting blood pressure will contribute to the different degrees of heart disease (ranging from 0 being healthy to 4 being the most severe heart cases).**\n",
    "\n",
    "\n",
    "The data we will use comes from ‚Äúprocessed.switzerland.data,‚Äù \"processed.cleveland.data,\" \"processed.hungarian.data\" and \"processed.va.data\" provided in the Heart Disease dataset {cite}`dataset`. It was collected from the clinical and noninvasive test results of 143 patients undergoing angiography at the University Hospitals in Zurich and Basel, 303 patients undergoing angiography at the Cleveland Clinic in Cleveland, Ohio, 425 patients undergoing angiography at the Hungarian Institute of Cardiology in Bupadepest and 200 patients undergoing angiography at the Veterans Administration Medical Center in Long Beach, California {cite}`diagnosis_algorithm`. \n",
    "\n",
    "Below are the packages we loaded to complete our data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îÄ‚îÄ \u001b[1mAttaching packages\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m‚úî\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6      \u001b[32m‚úî\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.5 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mtibble \u001b[39m 3.1.8      \u001b[32m‚úî\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.10\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.1      \u001b[32m‚úî\u001b[39m \u001b[34mstringr\u001b[39m 1.4.1 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.3      \u001b[32m‚úî\u001b[39m \u001b[34mforcats\u001b[39m 0.5.2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îÄ‚îÄ \u001b[1mConflicts\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mreadr\u001b[39m::\u001b[32medition_get()\u001b[39m   masks \u001b[34mtestthat\u001b[39m::edition_get()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m        masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mis_null()\u001b[39m       masks \u001b[34mtestthat\u001b[39m::is_null()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m           masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mreadr\u001b[39m::\u001b[32mlocal_edition()\u001b[39m masks \u001b[34mtestthat\u001b[39m::local_edition()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mmatches()\u001b[39m       masks \u001b[34mtidyr\u001b[39m::matches(), \u001b[34mtestthat\u001b[39m::matches()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mTest passed\u001b[39m üòÄ\n",
      "\u001b[32mTest passed\u001b[39m üò∏\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îÄ‚îÄ \u001b[1mAttaching packages\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.0.0 ‚îÄ‚îÄ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m‚úî\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.1     \u001b[32m‚úî\u001b[39m \u001b[34mrsample     \u001b[39m 1.1.1\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mdials       \u001b[39m 1.1.0     \u001b[32m‚úî\u001b[39m \u001b[34mtune        \u001b[39m 1.0.1\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34minfer       \u001b[39m 1.0.4     \u001b[32m‚úî\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.2\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.0.1     \u001b[32m‚úî\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.0\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mparsnip     \u001b[39m 1.0.3     \u001b[32m‚úî\u001b[39m \u001b[34myardstick   \u001b[39m 1.1.0\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.4     \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îÄ‚îÄ \u001b[1mConflicts\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m  masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m    masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m   masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mis_null()\u001b[39m   masks \u001b[34mtestthat\u001b[39m::is_null()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m       masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mrsample\u001b[39m::\u001b[32mmatches()\u001b[39m masks \u001b[34mdplyr\u001b[39m::matches(), \u001b[34mtidyr\u001b[39m::matches(), \u001b[34mtestthat\u001b[39m::matches()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m  masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m    masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m‚Ä¢\u001b[39m Learn how to get started at \u001b[32mhttps://www.tidymodels.org/start/\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m414\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m6\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m‚îÄ‚îÄ\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[32mdbl\u001b[39m (6): diagnosis_f, age, rest_bp, max_heart_rate, chest_pain, sex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[36m‚Ñπ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36m‚Ñπ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "source('../tests/tests.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# STEP IN 01 R SCRIPT\n",
    "\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(cowplot)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "options(repr.matrix.max.rows = 6)\n",
    "set.seed(1)\n",
    "source(\"../R/selection_forward_function.R\")\n",
    "source(\"../R/majority_classifier_function.R\")\n",
    "\n",
    "# Reading data off internet to make csv files\n",
    "source(\"../R/place_data.R\")\n",
    "source(\"../R/joining_data.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combined the datasets ‚Äúprocessed.switzerland.data,‚Äù \"processed.va.data\", \"processed.cleveland.data,\" and \"processed.hungarian.data\", to generate a longer and more complete dataset. Our complete aggregated data contains 13 different columns with an additional 8 that repeat some of the variables as factors instead of numeric types. Based on initial viewing of the data we decided to eliminate the `ST_dep` and `slope` columns as they were missing too many points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# STEP IN 02 R SCRIPT\n",
    "\n",
    "\n",
    "heart_data <- join_csv()\n",
    "heart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 1.*** *All columns with all data from the four heart disease data sets.*</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating our classification model, we partition the `heart_data` into a training (75%) and testing (25%) set using the `tidymodels` package. In data analysis it is important to split the data right away to ensure that the classifier we build never sees the testing data. We will train the model using only the training set and test it on the test set once we build the $K$-nn classifier. We will use the variable `diagnosis_f` as our class label as this is what we seek to predict. \n",
    "\n",
    "We set the seed with the `set.seed` function in order to make the randomized processes throughout our analysis reproducible. A seed is a numerical starting value, which determines the sequence of random numbers R will generate. Throughout the analysis the `set.seed` function will be at the top of each cell that completes a randomizing action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# STEP IN 03 R SCRIPT\n",
    "\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "set.seed(1)\n",
    "heart_split<-initial_split(heart_data, prop=0.75, strata=diagnosis_f)\n",
    "heart_training<-training(heart_split)\n",
    "heart_testing<-testing(heart_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Research on Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin choosing which predictors to use for our classifier, we conducted literature research on columns in our data set to determine their relationship with heart disease. This literature research below helped us to build our initial research question and is later confirmed with the use of foward selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resting Blood Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High blood pressure has always been associated with heart attack and heart disease. High blood pressure can damage the arteries by forcing them to stay in a taut position. This decreases the vessels ability to flow blood around the body which can lead to heart disease {cite}`disease_control`. From this we found indications that higher blood pressure in individuals resulted in coronary heart disease {cite}`blood_pressure`. According to this research we expect those with higher resting blood pressure to have more severe heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aging can cause changes in the heart and blood vessels that may increase person's risk of developing cardiovascular disease. Aging to one's heart is asociated with a reduction in its function. Some of the age-associated changes in the heart might be (partially) reversed, by exercise or specific drugs, however, it remains unclear whether this results in any definite advantages for the individual {cite}`disease_review`. According to this research we expect older individuals to obtain heart disease of higher severity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Heart Rate Achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low maximum heart rate can be associated with an increased risk of cardiac disease. Older hearts have a harder time than younger hearts pumping blood throughout the body because the heart gets tired and weaker with age. This can cause a decrease in daily physcial movement because there the heart is not able to preform as it used to. It is suspected that with a lower rate of blood pumping through the heart, there is a greater chance of other factors decreasing heart health that lead to heart disease {cite}`maximal_heartrate`. According to this research we expect max heart rate will decrease as severity of heart disease increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Visualizations of Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# SCRIPTED IN 02.1-INITIAL_VISUALIZATION.R\n",
    "\n",
    "# Boxplot visualizations for each of our predictors and heart diagnosis\n",
    "heart_data <- read_csv('../data/processed/heart_data.csv')\n",
    "\n",
    "    \n",
    "boxplot_age <- grid_boxplot(heart_data, age, \"Age (years)\", \"A. Boxplot of degree of heart \\n disease in relation to patient's \\nage\")\n",
    "boxplot_rest_bp <- grid_boxplot(heart_data, rest_bp, \"Resting blood pressure (mmHg)\", \"B. Boxplot of degree of heart \\ndisease in relation to patient's \\nresting blood pressure\")\n",
    "boxplot_max_heart_rate <- grid_boxplot(heart_data, max_heart_rate, \"Maximum heart rate (BPM)\", \"C. Boxplot of degree of heart \\ndisease in relation to patient's \\nmaximum heart rate\")\n",
    "options(repr.plot.width = 20, repr.plot.height = 10)\n",
    "boxplots <- plot_grid(boxplot_age, boxplot_rest_bp, boxplot_max_heart_rate, ncol=3)\n",
    "show(boxplots)\n",
    "ggsave(\"../figures/boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Figure 1.***</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxplots in Figure 1. are our inital visualizations to see if there really is a correlation between severity of heart disease and our predictors. From looking at all 3 predictors, Figure 1A. `age`, Figure 1B. `rest_bp`, and Figure 1C. `max_heart_rate`, we can see that there is a range of weakly positive and weakly negative to no relationship between the different diagnosed severities of heart disease (`diagnosis_f`)and the predictors.\n",
    "\n",
    "Figure 1A. shows there is a slight positive relation between age and heart disease severity, meaning the older a patient is, the more severe their heart disease would be.\n",
    "\n",
    "Figure 1B. shows there is no clear relationship between resting blood pressure and how severe a patients heart disease is. All the medians at different levels of severity are relatively level.\n",
    "\n",
    "Figure 1C. shows there is a slight negative relationship between maximum heart rate and the severity of heart disease. This means a patient with no heart disease (0) or low severity (1) are able to acheive a higher maximum heart rate. This is because their heart is more healthy and able to pump blood efficiently when compared to a patient with high severity (4) heart disease.\n",
    "\n",
    "These visualizations are taken into consideration when making our classification model. We will see if these predictors used together will in fact give an accurate diagnosis for someone being examined for heart disease based on given dataset. We chose to proceed with these predictors because of our literature reseach support. In practice, these predictors are conditions that doctors use to give an **initial diagnosis** of heart disease. Our research question as to whether these predictors work to accurately predict the severity is analysed in the remainder of the project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the effectiveness of our chosen predictors on our data set, we used the process of forward selection. We chose to add this step to confirm that the choices of our predictors will work using this data, as Figure 1. failed to show any significant correlation.\n",
    "\n",
    "Forward selection is used to predict accuracy of a classifier that will be made using different predictors to create the model. Based on our literature research detailed above, we will be using forward selection on the variables age, resting blood pressure, and maximum heart rate. We made sure to use our training data set in the forward selection process so that the testing data is never seen by the classifier.\n",
    "\n",
    "Additionally we chose to run forward selection on chest pain type (`chest_pain`) and sex (`sex`) to get an idea of what our classifier would look like using more variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Creating data subsets for forward selection model using training data\n",
    "heart_data_subset<-heart_training%>%\n",
    "    select(diagnosis_f, age, rest_bp, max_heart_rate, chest_pain, sex) %>%\n",
    "    na.omit()\n",
    "\n",
    "# heart_data_subset\n",
    "write_csv(heart_data_subset, '../data/modelling/forward_selection_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a subset of our data to work with, we can run forward selection to produce a table of accuracies based on the number of predictors. This forward selection coding immitates the model that will be made further down in the report. A seed is set to ensure reproducible results. \n",
    "\n",
    "Due to the iterative nature of forward selection and the usage of 5 predictors this cell will take a while to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "accuracies <- forwardSelection(heart_data_subset)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 2.*** *Forward selection results*</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Table 2. above, we can see that the accuracy increases with every added predictor. To make this result more clear, a visualization of accuracy compared to the number of predictors is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# visualization of number of predictors and accuracy based on forward selection\n",
    "options(repr.plot.width = 7, repr.plot.height = 7)\n",
    "forward_visualization <- ggplot(accuracies, aes(x = size, y = accuracy)) +\n",
    "    geom_line() +\n",
    "    geom_point() +\n",
    "    labs(x = 'Number of predictors used',\n",
    "         y = 'Estimated accuracy using forward selection',\n",
    "         title = 'Number of different predictors compared \\nto the accuracy of classifier model') +\n",
    "    theme(text = element_text(size = 20)) +\n",
    "    ylim(c(0,1))\n",
    "\n",
    "forward_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Figure 2.***</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. shows us that all five of our predictors increase the estimated accuracy of our classifier. However we can also see that the accuracy plateau's at the fourth and fifth predictor (`chest_pain` and `sex`). \n",
    "\n",
    "According to this visualization our pre-chosen predictors should be accurate predictors for the severity of heart disease. It also tells us that with any more predictors our classifier would not be benefitted and with any less we would have drastically lower accuracy. \n",
    "\n",
    "To conclude, we chose resting blood pressure, age and maximum heart rate as our predictors for the severity of heart disease as seen in row 3 of Table 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing Data tidying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our indicators `age`, `max_heart_rate` and `rest_bp` to predict `diagnosis_f` have been chosen, we can further tidy our data by eliminating the unused columns. Additionally, all the rows containing NA were removed. Since our data has already been split into training and testing sets we will tidy both of the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting chosen predictors within training and testing data\n",
    "heart_training <- heart_training %>%\n",
    "    select(rest_bp, age, max_heart_rate, diagnosis_f) %>%\n",
    "    na.omit()\n",
    "heart_training\n",
    "\n",
    "# Adding training set to directory\n",
    "write_csv(heart_training, '../data/modelling/training_split.csv')\n",
    "\n",
    "heart_testing <- heart_testing %>%\n",
    "    select(rest_bp, age, max_heart_rate, diagnosis_f) %>%\n",
    "    na.omit()\n",
    "\n",
    "# Adding testing set to directory\n",
    "write_csv(heart_testing, '../data/modelling/testing_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 3.*** *Tidied data set containing our selected predictors we will use to determine the severity of heart disease in an individual*</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting our predictors we examined the distribution of our outcome variable. Below, we show the majority classifier data frame which displays the proportion of each outcome found in our training data as well as a visualization to accompany it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# majority classifier and visualization\n",
    "set.seed(1)\n",
    "total_rows<-nrow(heart_training)\n",
    "\n",
    "number_of_columns<-heart_training%>%\n",
    "    group_by(diagnosis_f)%>%\n",
    "    summarize(number=n())%>%\n",
    "    select(number)\n",
    "    \n",
    "\n",
    "majority_classifier<-heart_training%>%\n",
    "    group_by(diagnosis_f)%>%   \n",
    "    summarize(percent_outcomes=n()/total_rows*100)%>%\n",
    "     arrange(desc(percent_outcomes))%>%\n",
    "     bind_cols(number_of_columns)\n",
    "#slice(1)\n",
    "majority_classifier\n",
    "\n",
    "# write csv to data/modelling\n",
    "write_csv(majority_classifier, '../data/modelling/majority_classifier.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 4.*** *Majority classifier showing the number of people with each severity level of heart disease in the training set*</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "majority_classifier_vis_function(majority_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Figure 3.***</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4. and Figure 3. give insight into the proportion of patients from the training set with each level of heart disease. The most common diagnosis in our data set is severity 1 with 31%. This value sets a baseline accuracy that our model should exceed to be deemed an acceptable classifier. In other words, if our model can predict more accurately than simply picking the most likely outcome every time, then we are on the right track in terms of accuracy.\n",
    "\n",
    "Table 4. also shows us that none of the percent outcomes are over 50%, and the outcomes for severity 1, 2, 3, and 0 are within 15% of each other. This means our data set is distributed evenly enough to build a decent classifier model. Knowing the frequency of each outcome also enables us to set an upper bound on our number of neighbors which is something we will discuss in further detail in subsequent cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following steps can all be found in the function `classifier`**.\n",
    "\n",
    "Because $K$-nearest neighbors is sensitive to the scale of the predictors, we will do some preprocessing to standardize them. We use only our training data to create a recipe for our classifier. This recipe will compute the shift / scale values for each variable, of the data we input into our classifier. For now we will use our training data as this is part of training procedure and we want to ensure that our test data does not influence any aspect of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we we made a model for our classifier being sure to use only the training dataset. \n",
    "First using `tune()`, so that we can find the optimal $K$ (number of neighbors) for our classifier. With `weight_func` we specify that we want to use the *straight-line* distance as our measurement between the predictor and our new point. Finally, we must specify that we are conducting classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal $K$ value for our classifier we must now separate our training data into many pieces to test each $K$ we are looking at. Five fold cross validation is used to split the training data into 5 sets of data which is then split into 5 pieces. In each group one of the 5 pieces immitates a testing set and is used to test each $K$ value and determine different accuracies for each $K$. \n",
    "\n",
    "We stratified the data by `diagnosis_f` to make sure the new splits contain similar proportions of each of the 5 diagnosis we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run a `workflow()` to determine various accuracies found using different $K$ values. This workflow does the cross-validation work that was described above using our now split training data. This workflow will include our model and recipe that were formed above. We tested $K$ values from one to twenty-one because this is the lowest frequency of diagnoses (category 4 appeared 21 times in the training set). If we were to use a number of neighbors greater than twenty-one the majority voting used in $Knn$ classification would be skewed against diagnosis 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cross validation to find optimal K value\n",
    "source('../R/classification_model.R')\n",
    "\n",
    "heart_data_accuracies <- classifier(heart_training)\n",
    "heart_data_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 5.*** *Parameter values that help select a k value*</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this table easier to understand, we made a visualization of the mean column and the number of neighbours. The mean represents an accuracy estimate of the model when different $K$ values are used. The optimal $K$ value has the highest accuracy and has neighbouring $K$ values of similar accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# K visualization\n",
    "source('../R/model_visualization.R')\n",
    "knn_visualization(heart_data_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Figure 4.***</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the number of neighbours to $K$ = 2 will give the highest accuracy, as seen in Figure 4. The shape of Figure 4. is not what we expected to see because of the huge drop in the accuracy after $K$ = 2. This means that there is a high risk of losing model accuracy after $K$ = 2. Choosing $K$ = 1 would also work considering the accuracy; however, it is a small number and it could lead to overfitting our classifier.\n",
    "\n",
    "Figure 4. shows that choosing a $K$ higher than two results in a drastically lower estimated accuracy. We experimented with testing the accuracy of models with higher $K$ values, but they decreased the accuracy even further. For example, when we used three neighbours instead of two, the accuracy decreased by about 15%, and it continued to decrease the higher the $K$ value we used. We also checked for spikes at much higher K values, but the accuracy continued to level off around 0.40. To make this table easier to understand, we made a visualization of the mean column and the number of neighbours. The mean represents an accuracy estimate of the model when different $K$ values are used. The optimal $K$ value has the highest accuracy and has neighbouring $K$ values of similar accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our initial analysis in Figure 4. we decided to continue our analysis despite the suspicious shape of Figure 4. This next step pulls optimal the $K$ value based on the above cross validation to choose the appropriate number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Pulling optimal K value based on above cross validation\n",
    "set.seed(1)\n",
    "best_k<-heart_data_accuracies%>%\n",
    "    arrange(desc(mean))%>%\n",
    "    slice(2)%>%\n",
    "    pull(neighbors)\n",
    "# best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our optimal $K$ value we continued to build our classifier and started by building a new model using `best_k`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# New classifier model using optimal K values\n",
    "set.seed(1)\n",
    "heart_data_spec_final<- nearest_neighbor(weight_func=\"rectangular\", neighbors=best_k)%>%\n",
    "    set_engine(\"kknn\")%>%\n",
    "    set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize the classifier we now plug the recipe and model into a workflow so it can be used on other data in the future. This step fits the classifier to our training data therefore enabling it's predictive ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Final workflow for classifier using new model.\n",
    "set.seed(1)\n",
    "heart_data_final_fit<-workflow()%>%\n",
    "    add_recipe(heart_data_recipe)%>%\n",
    "    add_model(heart_data_spec_final)%>%\n",
    "    fit(data=heart_training)\n",
    "    \n",
    "#heart_data_final_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the test set to our workflow to test the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Testing our classifier using the testing set\n",
    "set.seed(1)\n",
    "heart_data_summary<-heart_data_final_fit%>%\n",
    "    predict(heart_testing)%>%\n",
    "    bind_cols(heart_testing)%>%\n",
    "    metrics(truth=diagnosis_f, estimate=.pred_class)%>%\n",
    "    filter(.metric == 'accuracy')\n",
    "heart_data_summary\n",
    "\n",
    "heart_data_predict <- heart_data_final_fit %>%\n",
    "    predict(heart_testing) %>%\n",
    "    bind_cols(heart_testing)\n",
    "\n",
    "write_csv(heart_data_predict, '../data/modelling/predict_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Table 6.*** *Model accuracy with testing set*</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is a reasonable number that shows that even though our predictors within our model produce a strange $K$-nearest neighbour graph, we still built a model that will diagnose patients with ~80% accuracy.\n",
    "\n",
    "We know this is a reasonable accuracy for our classifier from looking at Figure 3. As discussed in the preceding paragraph, we are looking for a classifier with higher accuracy than that of the majority classifier. In our case, the majority label made up ~30% of our dataset, so the accuracy of ~80% is more than double that of our majority label. Therefore, our classifier is reasonably good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the model we built, we also provide a confusion matrix; a table of predicted and correct labels,  using the `conf_mat` function. This enables us to easily see false positives and false negatives of diagnosis, as well as what was predicted accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap based on the confusion matrix was made to more easily understand what is being displayed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# confusion matrix heat map visualization\n",
    "source('../R/confusion_matrix.R')\n",
    "\n",
    "confusion_matrix(heart_data_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray\">***Figure 5.***</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix (Figure 5.) shows that 22 individuals were correctly diagnosed with no presence of heart disease, 39 were correctly diagnosed with low severity of heart disease, 22 were correctly diagnosed with medium severity of heart disease, 20 were correctly diagnosed with high severity of heart disease, and 4 were correctly diagnosed with extreme severity of heart disease. Therefore, the classifier labeled $22 + 39 + 21 + 20 + 4 = 106$ diagnoses correctly. This is a rather good result as the proportions of each heart disease class diagnosed correctly mirror the proportions seen in the dataset. Our classifier is therefore not favouring one diagnosis over the other.\n",
    "\n",
    "Unfortunately, the classifier also made errors and classified a total of 18 patients with a false positive, meaning the patient's heart disease was worse than it actually was. Moreover, the classifier labelled six patients with false negatives, meaning the patients' heart disease was actually worse than what was predicted. A discussion as to the false positive and false negative results is in the discussion section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The predictors, `age`, `max_heart_rate`, and `rest_bp`, have high accuracy and a good chance of accurately predicting a patient's heart disease severity. However, some results indicate that these predictors may not be the best in actual practice. For instance, Figure 1. shows that these predictors do not have significant correlations with a diagnosis of severity of heart disease. Additionally, there was a significant drop-off after two neighbours in our $K$ graph, proposing that our accuracy results could be due to luck. However, our confusion matrix reveals promising results.\n",
    "\n",
    "### Is this what you expected to find?\n",
    "\n",
    "We expected to find that the larger the numerical value of a patient's age and resting blood pressure and the lower the maximum heart rate, the greater at risk a person is to have worsened heart disease. We actually found that these predictors have a weak correlation with the outcome variable. However, our model still predicts heart disease severity accurately.\n",
    "\n",
    "From our confusion matrix in Figure 5., we were more likely to predict a false positive than a false negative. This is good because, with a health condition like heart disease that generates ranging severity, it is better to be given a false positive and be treated as if the disease were worse than be given a false negative and not be treated at all or to a lower care level than what is needed. A reason we see 18 false positives could be because of the predictors we chose to use in our model. We saw in Figure 1. that these predictors are not ideal because of the weak relationships they show with the severities of heart disease. This can also be said as to why we have 6 false negatives. However, considering that the accuracy of our model is good, this is a reasonable number of false negatives. These values for the false positives and false negatives also relate to our research question and conclusion that our selected predictors are primarily good at predicting the preliminary severity of a patient's heart disease. However, a diagnosis cannot be concluded off these factors alone. Unfortunately, false negatives and false positives are given in medicine in the real world, but from actual research and examples like we were able to display in our classification, it is more likely that a patient will have a proper diagnosis than a false positive or negative.\n",
    "\n",
    "### What impact could such findings have?\n",
    "\n",
    "This could give medical professionals an idea that these predictors are good initial tests to check the severity of someone's heart disease, but further testing should be done to have a more accurate diagnosis. The severity levels of heart disease (0 to 4) are hard to distinguish between for our selected predictors, but they can be good indicators of a person having heart disease. This information can also be given to Government health care facilities as a precaution to provide public awareness and send letters to older citizens to check with a doctor on the status of their heart health. Also, individuals who feel they want to watch their heart health can compare their resting blood pressure, age and maximum heart rate with this model to estimate if they have heart disease and to what severity.\n",
    "\n",
    "### What future Questions could this lead to?\n",
    "\n",
    "What further research needs to be done on the predictors (maximum heart rate, age and resting blood pressure) to see if they are actually good predictors of heart disease? How could heart disease be lessened in the future, or how can heart disease be prevented? Patients at what age and with what symptoms should be encouraged to take precautions and see a doctor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}